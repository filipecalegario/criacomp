### 25/04/2024

* [SabiÃ¡-2 | Maritaca-ai](https://www.maritaca.ai/sabia-2): para analisar os modelos existentes
* [AnimaÃ§Ã£o de desenhos | Instagram](https://www.instagram.com/reel/C6L4XGzOkRC/)
* [Paul Couvert on X: "Absolutely stunning. Llama-3 8B was the most powerful small AI model a few hours ago. Microsoft has just released Phi-3 Mini which is better in almost every benchmark. You can use it locally for free as it's open-source (details below): https://t.co/aLc6Ih6Aaf" / X](https://twitter.com/itsPaulAi/status/1782813897835311348)
* [elvis on X: "Snowflake casually releases Arctic, an open-source LLM (Apache 2.0 license.) that uses a unique Dense-MoE Hybrid transformer architecture. Arctic performs on par with Llama3 70B in enterprise metrics like coding (HumanEval+ &amp; MBPP+), SQL (Spider) and instruction following https://t.co/WrLljhLv2Y" / X](https://twitter.com/omarsar0/status/1783176059694821632)
* [Gradio on X: "ğŸ¤¯ A GUI for fine-tuning the Llama-3 models on a free T4 GPU! ğŸ‘‰ ğ‹ğ‹ğšğŒğ€ ğ…ğšğœğ­ğ¨ğ«ğ² ğŸš€ Two Llama-3-fine-tunes using LLaMA Factory are already available at @huggingface Hub -- check Llama3-8B-Chinese-Chat and Llama3-Chinese for details. https://t.co/yLs834JsFI" / X](https://twitter.com/Gradio/status/1783092840353931378)
* [LlamaIndex ğŸ¦™ on X: "Language Agent Tree Search ğŸ¤–ğŸŒ² As LLMs get faster, better, cheaper, developers will be able to compose agentic systems that are able to plan out an entire tree of possible futures, instead of just sequentially planning the next state (e.g. in ReAct). This is crucial for higher https://t.co/D9AcGC5Zoj" / X](https://twitter.com/llama_index/status/1783147291882443112)
* [Ate-a-Pi on X: "This is AI Itâ€™s so over https://t.co/8LzdjokJxH" / X](https://twitter.com/8teAPi/status/1783179956396437832)
* [Jake Dahn on X: "I feel like I just joined a cult https://t.co/qnh21wpl2a" / X](https://twitter.com/jakedahn/status/1782988065268551769) 
