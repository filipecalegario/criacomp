{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/filipecalegario/criacomp/blob/main/2024_1_CRIACOMP_Word_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "golEYAY3lHVj"
      },
      "source": [
        "# OpenAI Word Embeddings, Semantic Search\n",
        "\n",
        "Word embeddings are a way of representing words and phrases as vectors. They can be used for a variety of tasks, including semantic search, anomaly detection, and classification. In the video on OpenAI Whisper, I mentioned how words whose vectors are numerically similar are also similar in semantic meaning. In this tutorial, we will learn how to implement semantic search using OpenAI embeddings. Understanding the Embeddings concept will be crucial to the next several videos in this series since we will use it to build several practical applications.\n",
        "\n",
        "To get started, we will need to install and import OpenAI and input an API Key. We learned how to do this in [Video 3 of this series](https://www.youtube.com/watch?v=LWYgjcZye1c)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUMmdUS_LPkI"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "client = OpenAI(api_key = userdata.get('OPENAI_KEY'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KXdnqkoyK9H"
      },
      "source": [
        "# Read Data File Containing Words\n",
        "\n",
        "Now that we have configured OpenAI, let's start with a simple CSV file with familiar words. From here we'll build up to a more complex semantic search using sentences from the Fed speech. [Save the linked \"words.csv\" as a CSV](https://gist.github.com/hackingthemarkets/25240a55e463822d221539e79d91a8d0) and upload it to Google Colab. Once the file is uploaded, let's read it into a pandas dataframe using the code below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHJ-2gvfx9-J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('words.csv')\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwUiwvTmL71c"
      },
      "source": [
        "# Calculate Word Embeddings\n",
        "\n",
        "To use word embeddings for semantic search, you first compute the embeddings for a corpus of text using a word embedding algorithm. What does this mean? We are going to create a numerical representation of each of these words. To perform this computation, we'll use OpenAI's 'get_embedding' function.\n",
        "\n",
        "Since we have our words in a pandas dataframe, we can use \"apply\" to apply the get_embedding function to each row in the dataframe. We then store the calculated word embeddings in a new text file called \"word_embeddings.csv\" so that we don't have to call OpenAI again to perform these calculations."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(openai_client, input, model):\n",
        "  return openai_client.embeddings.create(input=input, model=model).data[0].embedding"
      ],
      "metadata": {
        "id": "cRJ75CHk3UDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVUez91kL5kY"
      },
      "outputs": [],
      "source": [
        "df['embedding'] = df['text'].apply(lambda x: get_embedding(client, x, 'text-embedding-3-small'))\n",
        "df.to_csv('word_embeddings.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2WjqR_PzMtg"
      },
      "source": [
        "# Semantic Search\n",
        "\n",
        "Now that we have our word embeddings stored, let's load them into a new dataframe and use it for semantic search. Since the 'embedding' in the CSV is stored as a string, we'll use apply() and to interpret this string as Python code and convert it to a numpy array so that we can perform calculations on it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yM6N30oYeWhs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('word_embeddings.csv')\n",
        "df['embedding'] = df['embedding'].apply(eval).apply(np.array)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCtyD-yZz-6W"
      },
      "source": [
        "Let's now prompt ourselves for a search term that isn't in the dataframe. We'll use word embeddings to perform a semantic search for the words that are most similar to the word we entered. I'll first try the word \"hot dog\". Then we'll come back and try the word \"yellow\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtyaOReqzzn3"
      },
      "outputs": [],
      "source": [
        "search_term = input('Enter a search term: ')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpFRGaAX0H82"
      },
      "source": [
        "Now that we have a search term, let's calculate an embedding or vector for that search term using the OpenAI get_embedding function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrm5cxdDzfgU"
      },
      "outputs": [],
      "source": [
        "# semantic search\n",
        "search_term_vector = get_embedding(client, search_term, \"text-embedding-3-small\")\n",
        "search_term_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVSNY0Ci0hB5"
      },
      "source": [
        " Once we have a vector representing that word, we can see how similar it is to other words in our dataframe by calculating the cosine similarity of our search term's word vector to each word embedding in our dataframe."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference: https://platform.openai.com/docs/guides/embeddings/use-cases"
      ],
      "metadata": {
        "id": "y7hL4A6B487u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
      ],
      "metadata": {
        "id": "6UMIUUSg8C_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhE3ATG80oAt"
      },
      "outputs": [],
      "source": [
        "df[\"similarities\"] = df['embedding'].apply(lambda x: cosine_similarity(x, search_term_vector))\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsqkYbCD05TC"
      },
      "source": [
        "# Sorting By Similarity\n",
        "\n",
        "Now that we have calculated the similarities to each term in our dataframe, we simply sort the similarity values to find the terms that are most similar to the term we searched for. Notice how the foods are most similar to \"hot dog\". Not only that, it puts fast food closer to hot dog. Also some colors are ranked closer to hot dog than others. Let's go back and try the word \"yellow\" and walk through the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_4bDAkOg2m7"
      },
      "outputs": [],
      "source": [
        "df.sort_values(\"similarities\", ascending=False).head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LnJhZLK2pgz"
      },
      "source": [
        "# Adding Words Together\n",
        "\n",
        "What's even more interesting is that we can add word vectors together. What happens when we add the numbers for milk and espresso, then search for the word vector most similar to milk + espresso? Let's make a copy of the original dataframe and call it food_df. We'll operate on this copy. Let's try adding word together. Let's add milk + espresso and store the results in milk_espresso_vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CD-_nVUhGBH"
      },
      "outputs": [],
      "source": [
        "food_df = df.copy()\n",
        "\n",
        "milk_vector = food_df['embedding'][10]\n",
        "espresso_vector = food_df['embedding'][19]\n",
        "\n",
        "milk_espresso_vector = milk_vector + espresso_vector\n",
        "milk_espresso_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's find the words most similar to milk + espresso. If you have never done this before, it's pretty surprising that you can add words together like this and find similar words using numbers."
      ],
      "metadata": {
        "id": "PxlPom9m4nLB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3UCdDmJjfaQ"
      },
      "outputs": [],
      "source": [
        "food_df[\"similarities\"] = food_df['embedding'].apply(lambda x: cosine_similarity(x, milk_espresso_vector))\n",
        "food_df.sort_values(\"similarities\", ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Microsoft Earnings Call Transcript\n",
        "\n",
        "Let's tie this back to finance. I have attached some text from a recent [Microsoft earnings call here](https://gist.github.com/hackingthemarkets/1c827a7750384fcf52c84594ef216a2d). Click on \"raw\" and save the file as a CSV. Upload it to Google Colab as microsoft-earnings.csv. Let's use what we just learned to perform a semantic search on sentences in the Microsoft earnings call. We'll start by reading the paragraphs into a pandas dataframe."
      ],
      "metadata": {
        "id": "ron5G1UP5EeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "earnings_df = pd.read_csv('microsoft-earnings.csv')\n",
        "earnings_df"
      ],
      "metadata": {
        "id": "8L9u-1Mp5iVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have the dataframe, we'll once again compute the embeddings for each line in our CSV file."
      ],
      "metadata": {
        "id": "C8RpdYA_5vIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "earnings_df['embedding'] = earnings_df['text'].apply(lambda x: get_embedding(x, engine='text-embedding-ada-002'))\n",
        "earnings_df.to_csv('earnings-embeddings.csv')"
      ],
      "metadata": {
        "id": "jQT2IXJP5zFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you download the earnings_embeddings.csv file locally and open it up, you'll see that our embeddings are for entire paragraphs - not just words. This means that we'll be able to search on similar sentences even if there isn't an exact match for the string we search for. We are searching on meaning."
      ],
      "metadata": {
        "id": "E0G-GDdG6ABW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "earnings_search = input(\"Search earnings for a sentence:\")"
      ],
      "metadata": {
        "id": "ghd9XP0P6VCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "earnings_search_vector = get_embedding(earnings_search, engine=\"text-embedding-ada-002\")\n",
        "earnings_search_vector"
      ],
      "metadata": {
        "id": "z42fW6_G6SeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "earnings_df[\"similarities\"] = earnings_df['embedding'].apply(lambda x: cosine_similarity(x, earnings_search_vector))\n",
        "\n",
        "earnings_df\n"
      ],
      "metadata": {
        "id": "cxgaKmPx6qty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "earnings_df.sort_values(\"similarities\", ascending=False)"
      ],
      "metadata": {
        "id": "19W1ZT1f61kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVYN-f9O3Dh7"
      },
      "source": [
        "# Sentences of the Fed Speech\n",
        "\n",
        "Let's use the Fed Speech example once more. Let's calculate the word embeddings for a particular sentence in the November 2nd speech that we discussed in the OpenAI Whisper tutorial. Then we'll take a new sentence from a future speech that isn't in our dataset, and find the most similar sentence in our dataset. Here is the sentence we will use to search for similarity:\n",
        "\n",
        "\"the inflation is too damn high\"\n",
        "\n",
        "As we did previously, take [the linked CSV file](https://gist.github.com/hackingthemarkets/9b55ea8b73c7f4e04b42a9f8eddb8393) and upload it to Google Colab as fed-speech.csv. We'll once again read it into a pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqNv1M35l3DL"
      },
      "outputs": [],
      "source": [
        "fed_df = pd.read_csv('fed-speech.csv')\n",
        "fed_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll once again calculate the embeddings and save them in a new CSV file."
      ],
      "metadata": {
        "id": "T3k0RG-C9bcL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTOTIpJ6406-"
      },
      "outputs": [],
      "source": [
        "fed_df['embedding'] = fed_df['text'].apply(lambda x: get_embedding(x, engine='text-embedding-ada-002'))\n",
        "fed_df.to_csv('fed-embeddings.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll then enter the new sentence that we want to find similarity for:\n",
        "\n",
        "\"We will continue to increase interest rates and tighten monetary policy\""
      ],
      "metadata": {
        "id": "vNB3ERqi9gtZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIMR6NGA4Nib"
      },
      "outputs": [],
      "source": [
        "fed_sentence = input('Enter something Jerome Powell said: ')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again we'll get the vector for this sentence, find the cosine similarity, and sort by most similar."
      ],
      "metadata": {
        "id": "hdmc5k1G9nLR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RV9TNBKM4eiA"
      },
      "outputs": [],
      "source": [
        "fed_sentence_vector = get_embedding(fed_sentence, engine=\"text-embedding-ada-002\")\n",
        "fed_sentence_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "245b-TFV4k-a"
      },
      "outputs": [],
      "source": [
        "fed_df = pd.read_csv('fed-embeddings.csv')\n",
        "fed_df['embedding'] = fed_df['embedding'].apply(eval).apply(np.array)\n",
        "fed_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHLURVqp5k56"
      },
      "outputs": [],
      "source": [
        "\n",
        "fed_df[\"similarities\"] = fed_df['embedding'].apply(lambda x: cosine_similarity(x, fed_sentence_vector))\n",
        "\n",
        "fed_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rfb4qoc4rVL"
      },
      "outputs": [],
      "source": [
        "\n",
        "fed_df.sort_values(\"similarities\", ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating Cosine Similarity\n",
        "\n",
        "We used the Cosine Similarity function, but how does it actually work? Cosine similarity is just calculating the similarity between two vectors. There is a mathematical equation for calculating the angle between two vectors.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1cehvtx7LKuFeq_LqfnLi-gzIz1D1wSf9)"
      ],
      "metadata": {
        "id": "CC0UgmJeCM36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "v1 = np.array([1,2,3])\n",
        "v2 = np.array([4,5,6])\n",
        "\n",
        "# (1 * 4) + (2 * 5) + (3 * 6)\n",
        "dot_product = np.dot(v1, v2)\n",
        "dot_product"
      ],
      "metadata": {
        "id": "ws7hh97DATIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# square root of (1^2 + 2^2 + 3^2) = square root of (1+4+9) = square root of 14\n",
        "np.linalg.norm(v1)"
      ],
      "metadata": {
        "id": "pz18oXLhBhs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# square root of (4^2 + 5^2 + 6^2) = square root of (16+25+36) = square root of 14\n",
        "np.linalg.norm(v2)"
      ],
      "metadata": {
        "id": "RHpa6BhjCA1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "magnitude = np.linalg.norm(v1) * np.linalg.norm(v2)\n",
        "magnitude"
      ],
      "metadata": {
        "id": "VuxTawPcAYGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dot_product / magnitude"
      ],
      "metadata": {
        "id": "qIyzZ3vABa5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import spatial\n",
        "\n",
        "result = 1 - spatial.distance.cosine(v1, v2)\n",
        "\n",
        "result"
      ],
      "metadata": {
        "id": "n5Ppa9jsBAU1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}